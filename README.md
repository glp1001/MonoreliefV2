<div align="center">
<h1>MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery
</h1>

[**Yu-Wei Zhang**]<sup>1*&dagger;</sup> · [**Tongju Han**]<sup>1</sup> · [**Lipeng Gao**]<sup>1</sup>
<br>
[**Mingqiang Wei**]<sup>2</sup> · [**Hui Liu**]<sup>3</sup> · [**Changbao Li**]<sup>1</sup> · [**Caiming Zhang**]<sup>4</sup>

<sup>1</sup>QLU&emsp;&emsp;&emsp;<sup>2</sup>NUAA;<sup>3</sup>SDUFE;<sup>4</sup>SDU
<br>
&dagger;project lead&emsp;*corresponding author



This work presents Depth Anything V2. It significantly outperforms [V1](https://github.com/LiheYoung/Depth-Anything) in fine-grained details and robustness. Compared with SD-based models, it enjoys faster inference speed, fewer parameters, and higher depth accuracy.

![teaser](assets/teaser.png)


## News
- **2025-01-22:** [Video Depth Anything](https://videodepthanything.github.io) has been released. It generates consistent depth maps for super-long videos (e.g., over 5 minutes).
